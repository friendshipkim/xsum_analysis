{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd23f2ae",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bad6821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from xsum_dataset import XsumDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93660df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset xsum (/home/wk247/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcfed792653c469ca05de1a06c373cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xsum_data_raw = datasets.load_dataset(\"xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d98f3f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/val/test data\n",
    "xsum_train_data = XsumDataset(xsum_data_raw[\"train\"])\n",
    "xsum_val_data = XsumDataset(xsum_data_raw[\"validation\"])\n",
    "xsum_test_data = XsumDataset(xsum_data_raw[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae425a5f",
   "metadata": {},
   "source": [
    "# concat data\n",
    "xsum_data_raw_cc = datasets.concatenate_datasets(\n",
    "    [xsum_data_raw[\"train\"], xsum_data_raw[\"validation\"], xsum_data_raw[\"test\"]]\n",
    "    )\n",
    "xsum_concat_data = XsumDataset(xsum_data_raw_cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dc9e79",
   "metadata": {},
   "source": [
    "# NER\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6487896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12f8881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#NER = spacy.load(\"en_core_web_lg\")\n",
    "NER = spacy.load(\"en_core_web_trf\")\n",
    "cache_dir = \"../cache_trf\"\n",
    "\n",
    "# from collections import Counter\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5154911f",
   "metadata": {},
   "source": [
    "## 1) create ner / entities list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb707b0",
   "metadata": {},
   "source": [
    "1) train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1da46e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                        | 24/204045 [00:10<25:41:22,  2.21it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _, _, train_doc_ents_list, train_sum_ents_list \u001b[38;5;241m=\u001b[39m \u001b[43mtag_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtagger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43mxsum_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxsum_train_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43mreturn_ner_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/xsum_analysis/ner_utils.py:31\u001b[0m, in \u001b[0;36mtag_dataset\u001b[0;34m(tagger, xsum_data, return_ner_list)\u001b[0m\n\u001b[1;32m     28\u001b[0m true_summary \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# ner\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m doc_ner \u001b[38;5;241m=\u001b[39m \u001b[43mtagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# removing newline changes the ner result\u001b[39;00m\n\u001b[1;32m     32\u001b[0m sum_ner \u001b[38;5;241m=\u001b[39m tagger(true_summary)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_ner_list: \u001b[38;5;66;03m# train data is too large to store all ner objects\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/xsum_analysis/lib/python3.8/site-packages/spacy/language.py:1017\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1015\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1017\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1019\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/xsum_analysis/lib/python3.8/site-packages/spacy_transformers/pipeline_component.py:192\u001b[0m, in \u001b[0;36mTransformer.__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m\"\"\"Apply the pipe to one document. The document is modified in place,\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03mand returned. This usually happens under the hood when the nlp object\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03mis called on a text and all components are applied to the Doc.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mDOCS: https://spacy.io/api/transformer#call\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m install_extensions()\n\u001b[0;32m--> 192\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_annotations([doc], outputs)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[0;32m~/.conda/envs/xsum_analysis/lib/python3.8/site-packages/spacy_transformers/pipeline_component.py:228\u001b[0m, in \u001b[0;36mTransformer.predict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    226\u001b[0m     activations \u001b[38;5;241m=\u001b[39m FullTransformerBatch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(docs))\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 228\u001b[0m     activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m batch_id \u001b[38;5;241m=\u001b[39m TransformerListener\u001b[38;5;241m.\u001b[39mget_batch_id(docs)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m listener \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlisteners:\n",
      "File \u001b[0;32m~/.conda/envs/xsum_analysis/lib/python3.8/site-packages/thinc/model.py:315\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/xsum_analysis/lib/python3.8/site-packages/spacy_transformers/layers/transformer_model.py:185\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, docs, is_train)\u001b[0m\n\u001b[1;32m    181\u001b[0m align \u001b[38;5;241m=\u001b[39m get_alignment(flat_spans, wordpieces\u001b[38;5;241m.\u001b[39mstrings, tokenizer\u001b[38;5;241m.\u001b[39mall_special_tokens)\n\u001b[1;32m    182\u001b[0m wordpieces, align \u001b[38;5;241m=\u001b[39m truncate_oversize_splits(\n\u001b[1;32m    183\u001b[0m     wordpieces, align, tokenizer\u001b[38;5;241m.\u001b[39mmodel_max_length\n\u001b[1;32m    184\u001b[0m )\n\u001b[0;32m--> 185\u001b[0m model_output, bp_tensors \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwordpieces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogger\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mattrs:\n\u001b[1;32m    187\u001b[0m     log_gpu_memory(model\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogger\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafter forward\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/xsum_analysis/lib/python3.8/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xsum_analysis/lib/python3.8/site-packages/thinc/layers/pytorchwrapper.py:134\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m    131\u001b[0m convert_outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    133\u001b[0m Xtorch, get_dX \u001b[38;5;241m=\u001b[39m convert_inputs(model, X, is_train)\n\u001b[0;32m--> 134\u001b[0m Ytorch, torch_backprop \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshims\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtorch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m Y, get_dYtorch \u001b[38;5;241m=\u001b[39m convert_outputs(model, (X, Ytorch), is_train)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dY: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[0;32m~/.conda/envs/xsum_analysis/lib/python3.8/site-packages/thinc/shims/pytorch.py:72\u001b[0m, in \u001b[0;36mPyTorchShim.__call__\u001b[0;34m(self, inputs, is_train)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbegin_update(inputs)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mlambda\u001b[39;00m a: \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/xsum_analysis/lib/python3.8/site-packages/thinc/shims/pytorch.py:82\u001b[0m, in \u001b[0;36mPyTorchShim.predict\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m amp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mixed_precision):\n\u001b[0;32m---> 82\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.conda/envs/xsum_analysis/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/xsum_analysis/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:850\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    841\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    843\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    844\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    845\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    848\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    849\u001b[0m )\n\u001b[0;32m--> 850\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    863\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/xsum_analysis/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/xsum_analysis/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:526\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    517\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    518\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    519\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 526\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.conda/envs/xsum_analysis/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/xsum_analysis/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:453\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    450\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    451\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 453\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/xsum_analysis/lib/python3.8/site-packages/transformers/modeling_utils.py:2438\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   2435\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m   2436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m-> 2438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xsum_analysis/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:466\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    465\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 466\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/.conda/envs/xsum_analysis/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/xsum_analysis/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:378\u001b[0m, in \u001b[0;36mRobertaOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, input_tensor):\n\u001b[0;32m--> 378\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    380\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/.conda/envs/xsum_analysis/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/xsum_analysis/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xsum_analysis/lib/python3.8/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_, _, train_doc_ents_list, train_sum_ents_list = tag_dataset(tagger=NER,\n",
    "                                                             xsum_dataset=xsum_train_data.dataset, \n",
    "                                                             return_ner_list=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c8ee405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to '../cache/train_doc_ents_list.pkl'\n",
      "saved to '../cache/train_sum_ents_list.pkl'\n"
     ]
    }
   ],
   "source": [
    "# save to cache directory\n",
    "save_to_cache_dir(\n",
    "    train_doc_ents_list, \n",
    "    \"train_doc_ents_list\",\n",
    "    cache_dir)\n",
    "\n",
    "save_to_cache_dir(\n",
    "    train_sum_ents_list, \n",
    "    \"train_sum_ents_list\",\n",
    "    cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad85fa21",
   "metadata": {},
   "source": [
    "2) val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387a10a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, val_doc_ents_list, val_sum_ents_list = tag_dataset(tagger=NER,\n",
    "                                                         xsum_dataset=xsum_val_data.dataset, \n",
    "                                                         return_ner_list=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228518e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_doc_ner_list, val_sum_ner_list, val_doc_ents_list, val_sum_ents_list = tag_dataset(tagger=NER,\n",
    "                                                         xsum_dataset=xsum_val_data.dataset, \n",
    "                                                         return_ner_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5da44103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to '../cache_trf/val_doc_ents_list.pkl'\n",
      "saved to '../cache_trf/val_sum_ents_list.pkl'\n"
     ]
    }
   ],
   "source": [
    "# save_to_cache_dir(val_doc_ner_list, \"val_doc_ner_list\")\n",
    "# save_to_cache_dir(val_sum_ner_list, \"val_sum_ner_list\")\n",
    "save_to_cache_dir(val_doc_ents_list, \"val_doc_ents_list\")\n",
    "save_to_cache_dir(val_sum_ents_list, \"val_sum_ents_list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4741dbb",
   "metadata": {},
   "source": [
    "3) test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba68cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, test_doc_ents_list, test_sum_ents_list = tag_dataset(tagger=NER,\n",
    "                                                         xsum_data=xsum_test_data, \n",
    "                                                         return_ner_list=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58c3b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc_ner_list, test_sum_ner_list, test_doc_ents_list, test_sum_ents_list = tag_dataset(tagger=NER,\n",
    "                                                         xsum_data=xsum_test_data, \n",
    "                                                         return_ner_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7777b367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to '../cache_trf/test_doc_ents_list.pkl'\n",
      "saved to '../cache_trf/test_sum_ents_list.pkl'\n"
     ]
    }
   ],
   "source": [
    "# save_to_cache_dir(test_doc_ner_list, \"test_doc_ner_list\")\n",
    "# save_to_cache_dir(test_sum_ner_list, \"test_sum_ner_list\")\n",
    "save_to_cache_dir(test_doc_ents_list, \"test_doc_ents_list\")\n",
    "save_to_cache_dir(test_sum_ents_list, \"test_sum_ents_list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d69c5a",
   "metadata": {},
   "source": [
    "## 2) create the pool of entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca29002",
   "metadata": {},
   "source": [
    "* label list\n",
    "    * CARDINAL: Numerals that do not fall under another type\n",
    "    * DATE: Absolute or relative dates or periods\n",
    "    * EVENT: Named hurricanes, battles, wars, sports events, etc.\n",
    "    * FAC: Buildings, airports, highways, bridges, etc.\n",
    "    * GPE: Countries, cities, states\n",
    "    * LANGUAGE: Any named language\n",
    "    * LAW: Named documents made into laws.\n",
    "    * LOC: Non-GPE locations, mountain ranges, bodies of water\n",
    "    * MONEY: Monetary values, including unit\n",
    "    * NORP: Nationalities or religious or political groups\n",
    "    * ORDINAL: \"first\", \"second\", etc.\n",
    "    * ORG: Companies, agencies, institutions, etc.\n",
    "    * PERCENT: Percentage, including \"%\"\n",
    "    * PERSON: People, including fictional\n",
    "    * PRODUCT: Objects, vehicles, foods, etc. (not services)\n",
    "    * QUANTITY: Measurements, as of weight or distance\n",
    "    * TIME: Times smaller than a day\n",
    "    * WORK_OF_ART: Titles of books, songs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d6e9e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_LABELS = list(NER.get_pipe('ner').labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fac0d9",
   "metadata": {},
   "source": [
    "1) test entities pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63c045c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'../cache_trf/test_doc_ents_list.pkl' loaded\n"
     ]
    }
   ],
   "source": [
    "# # load entities list\n",
    "test_doc_ents_list = load_from_cache_dir(\"test_doc_ents_list\", cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e557550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 11334/11334 [00:34<00:00, 326.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: CARDINAL, count: 5114\n",
      "label: DATE, count: 10843\n",
      "label: EVENT, count: 1955\n",
      "label: FAC, count: 4515\n",
      "label: GPE, count: 5965\n",
      "label: LANGUAGE, count: 83\n",
      "label: LAW, count: 645\n",
      "label: LOC, count: 1612\n",
      "label: MONEY, count: 4102\n",
      "label: NORP, count: 1338\n",
      "label: ORDINAL, count: 175\n",
      "label: ORG, count: 20547\n",
      "label: PERCENT, count: 1402\n",
      "label: PERSON, count: 40806\n",
      "label: PRODUCT, count: 1552\n",
      "label: QUANTITY, count: 2459\n",
      "label: TIME, count: 3429\n",
      "label: WORK_OF_ART, count: 3222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_ent_pool_dict = create_ent_pool_dict(test_doc_ents_list, ALL_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99a67e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to '../cache_trf/test_ent_pool_dict.pkl'\n"
     ]
    }
   ],
   "source": [
    "save_to_cache_dir(test_ent_pool_dict, \"test_ent_pool_dict\", cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3d890b",
   "metadata": {},
   "source": [
    "2) concat entities pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ae3dd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'../cache_trf/val_doc_ents_list.pkl' loaded\n",
      "'../cache_trf/test_doc_ents_list.pkl' loaded\n"
     ]
    }
   ],
   "source": [
    "# # load entities lists\n",
    "# train_doc_ents_list = load_from_cache_dir(\"train_doc_ents_list\", cache_dir=cache_dir)\n",
    "val_doc_ents_list = load_from_cache_dir(\"val_doc_ents_list\", cache_dir=cache_dir)\n",
    "test_doc_ents_list = load_from_cache_dir(\"test_doc_ents_list\", cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84327d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22666\n"
     ]
    }
   ],
   "source": [
    "concat_doc_ents_list = [*val_doc_ents_list, *test_doc_ents_list]  # *train_doc_ents_list, \n",
    "print(len(concat_doc_ents_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db795a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 22666/22666 [02:25<00:00, 155.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: CARDINAL, count: 8328\n",
      "label: DATE, count: 17273\n",
      "label: EVENT, count: 3322\n",
      "label: FAC, count: 8347\n",
      "label: GPE, count: 9341\n",
      "label: LANGUAGE, count: 113\n",
      "label: LAW, count: 1172\n",
      "label: LOC, count: 2719\n",
      "label: MONEY, count: 7430\n",
      "label: NORP, count: 1920\n",
      "label: ORDINAL, count: 229\n",
      "label: ORG, count: 34501\n",
      "label: PERCENT, count: 2160\n",
      "label: PERSON, count: 67832\n",
      "label: PRODUCT, count: 2672\n",
      "label: QUANTITY, count: 4329\n",
      "label: TIME, count: 5742\n",
      "label: WORK_OF_ART, count: 5809\n"
     ]
    }
   ],
   "source": [
    "concat_ent_pool_dict = create_ent_pool_dict(concat_doc_ents_list, ALL_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "488aaa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to '../cache_trf/val_test_ent_pool_dict.pkl'\n"
     ]
    }
   ],
   "source": [
    "save_to_cache_dir(concat_ent_pool_dict, \"val_test_ent_pool_dict\", cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6f93d5",
   "metadata": {},
   "source": [
    "### 2-1) Preprocess entity pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "298d9952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3546ac3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'../cache/concat_ent_pool_dict.pkl' loaded\n"
     ]
    }
   ],
   "source": [
    "# entities pool\n",
    "concat_ent_pool_dict = load_from_cache_dir(\"concat_ent_pool_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65ea82d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_LABELS = [\"PERSON\", \"FAC\", \"GPE\", \"NORP\", \"LOC\", \"EVENT\", \"LANGUAGE\", \"LAW\", \"ORG\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07c66dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_LABELS = list(NER.get_pipe('ner').labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f809e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= PERSON =========\n",
      "ent: David Cameron, count:367\n",
      "ent: Trump, count:336\n",
      "ent: Donald Trump, count:283\n",
      "ent: Jones, count:259\n",
      "ent: Theresa May, count:258\n",
      "ent: Barack Obama, count:235\n",
      "ent: Obama, count:234\n",
      "ent: Cameron, count:222\n",
      "ent: Williams, count:194\n",
      "ent: Smith, count:192\n",
      "ent: Johnson, count:166\n",
      "ent: George Osborne, count:158\n",
      "ent: May, count:154\n",
      "ent: Jeremy Corbyn, count:150\n",
      "ent: Davies, count:150\n",
      "ent: Taylor, count:142\n",
      "ent: Vladimir Putin, count:139\n",
      "ent: James, count:134\n",
      "ent: Nicola Sturgeon, count:132\n",
      "ent: Hillary Clinton, count:130\n",
      "ent: Corbyn, count:129\n",
      "ent: Brown, count:127\n",
      "ent: Murray, count:122\n",
      "ent: Evans, count:112\n",
      "ent: Hamilton, count:110\n",
      "ent: Lewis, count:110\n",
      "ent: David, count:107\n",
      "ent: Boris Johnson, count:106\n",
      "ent: Bashar al-Assad, count:102\n",
      "ent: Putin, count:94\n",
      "ent: Morgan, count:93\n",
      "ent: Angela Merkel, count:93\n",
      "ent: Wilson, count:93\n",
      "ent: Carwyn Jones, count:92\n",
      "ent: Cook, count:91\n",
      "ent: John, count:91\n",
      "ent: Hughes, count:88\n",
      "ent: Anderson, count:88\n",
      "ent: Ed Miliband, count:87\n",
      "ent: Clinton, count:87\n",
      "ent: Michael, count:86\n",
      "ent: Thomas, count:79\n",
      "ent: Wood, count:78\n",
      "ent: Martin, count:77\n",
      "ent: Clarke, count:77\n",
      "ent: Nigel Farage, count:75\n",
      "ent: Paul, count:75\n",
      "ent: Roberts, count:74\n",
      "ent: Cooper, count:72\n",
      "ent: Osborne, count:72\n",
      "ent: Andy Murray, count:71\n",
      "ent: Miller, count:71\n",
      "ent: Thompson, count:71\n",
      "ent: Francois Hollande, count:70\n",
      "ent: Sturgeon, count:69\n",
      "ent: Clark, count:69\n",
      "ent: Jose Mourinho, count:67\n",
      "ent: Khan, count:66\n",
      "ent: Mark, count:66\n",
      "ent: Watson, count:66\n",
      "ent: Lee, count:65\n",
      "ent: Assad, count:65\n",
      "ent: Murphy, count:63\n",
      "ent: Martin McGuinness, count:63\n",
      "ent: Philip Hammond, count:62\n",
      "ent: Scott, count:62\n",
      "ent: Alastair Cook, count:61\n",
      "ent: Recep Tayyip Erdogan, count:61\n",
      "ent: Robinson, count:60\n",
      "ent: Nick Clegg, count:60\n",
      "ent: Arlene Foster, count:60\n",
      "ent: O'Brien, count:59\n",
      "ent: John Kerry, count:59\n",
      "ent: Green, count:59\n",
      "ent: George, count:58\n",
      "ent: Mourinho, count:57\n",
      "ent: Adams, count:57\n",
      "ent: Walker, count:57\n",
      "ent: Francis, count:57\n",
      "ent: Foster, count:57\n",
      "ent: King, count:56\n",
      "ent: Moore, count:56\n",
      "ent: Wright, count:56\n",
      "ent: Allen, count:55\n",
      "ent: Wayne Rooney, count:54\n",
      "ent: Joe Root, count:54\n",
      "ent: Stewart, count:54\n",
      "ent: Xi Jinping, count:53\n",
      "ent: Davis, count:53\n",
      "ent: Andy Burnham, count:52\n",
      "ent: Hitler, count:52\n",
      "ent: Bell, count:52\n",
      "ent: David Cameron's, count:52\n",
      "ent: Michael Gove, count:52\n",
      "ent: Andy, count:52\n",
      "ent: Rodgers, count:51\n",
      "ent: Chris, count:51\n",
      "ent: Peter, count:51\n",
      "ent: Sadiq Khan, count:50\n",
      "ent: Ward, count:50\n",
      "\n",
      "========= FAC =========\n",
      "ent: Wembley, count:151\n",
      "ent: Old Trafford, count:104\n",
      "ent: the White House, count:50\n",
      "ent: Lord's, count:49\n",
      "ent: Anfield, count:47\n",
      "ent: Twickenham, count:46\n",
      "ent: Heathrow, count:43\n",
      "ent: High Street, count:42\n",
      "ent: Stamford Bridge, count:42\n",
      "ent: Westminster, count:40\n",
      "ent: the Old Bailey, count:36\n",
      "ent: Edgbaston, count:34\n",
      "ent: Easter Road, count:32\n",
      "ent: Celtic Park, count:31\n",
      "ent: Goodison Park, count:29\n",
      "ent: Vatican, count:28\n",
      "ent: the International Space Station, count:28\n",
      "ent: The Oval, count:27\n",
      "ent: Centre Court, count:27\n",
      "ent: M4, count:26\n",
      "ent: Buckingham Palace, count:26\n",
      "ent: Downing Street, count:25\n",
      "ent: Rugby Park, count:24\n",
      "ent: Trent Bridge, count:24\n",
      "ent: Grenfell Tower, count:24\n",
      "ent: Wembley Stadium, count:23\n",
      "ent: Murrayfield, count:22\n",
      "ent: Heathrow Airport, count:22\n",
      "ent: St James' Park, count:22\n",
      "ent: Broadway, count:22\n",
      "ent: the Liberty Stadium, count:21\n",
      "ent: Guantanamo Bay, count:20\n",
      "ent: Trump Tower, count:20\n",
      "ent: HS2, count:19\n",
      "ent: Rodney Parade, count:19\n",
      "ent: White Hart Lane, count:19\n",
      "ent: M1, count:19\n",
      "ent: London Bridge, count:18\n",
      "ent: the Olympic Stadium, count:18\n",
      "ent: Headingley, count:18\n",
      "ent: ISS, count:17\n",
      "ent: Auschwitz, count:17\n",
      "ent: Gatwick, count:17\n",
      "ent: Elland Road, count:16\n",
      "ent: M25, count:16\n",
      "ent: Queen Street, count:16\n",
      "ent: St Mary's, count:16\n",
      "ent: Ibrox, count:16\n",
      "ent: Carrow Road, count:16\n",
      "ent: the Ricoh Arena, count:15\n",
      "ent: The Valley, count:15\n",
      "ent: Upton Park, count:15\n",
      "ent: Ashton Gate, count:15\n",
      "ent: Crucible, count:15\n",
      "ent: Tube, count:15\n",
      "ent: New Road, count:15\n",
      "ent: Windsor Park, count:14\n",
      "ent: Hampden Park, count:14\n",
      "ent: Roland Garros, count:14\n",
      "ent: Edinburgh Airport, count:14\n",
      "ent: Augusta, count:14\n",
      "ent: George Square, count:13\n",
      "ent: Glasgow Airport, count:13\n",
      "ent: St Mary's Church, count:13\n",
      "ent: Manchester Airport, count:13\n",
      "ent: Windsor Castle, count:13\n",
      "ent: Cardiff Central, count:13\n",
      "ent: Emirates, count:13\n",
      "ent: the City Ground, count:13\n",
      "ent: the Channel Tunnel, count:13\n",
      "ent: the Millennium Stadium, count:13\n",
      "ent: London Road, count:13\n",
      "ent: Fir Park, count:13\n",
      "ent: Victoria Park, count:13\n",
      "ent: Silverstone, count:13\n",
      "ent: Etihad Stadium, count:12\n",
      "ent: the Stade de France, count:12\n",
      "ent: Loftus Road, count:12\n",
      "ent: Hillsborough, count:12\n",
      "ent: Guantanamo, count:12\n",
      "ent: Manchester Arena, count:12\n",
      "ent: the Principality Stadium, count:12\n",
      "ent: Croke Park, count:12\n",
      "ent: Ewood Park, count:12\n",
      "ent: Trafalgar Square, count:12\n",
      "ent: Queen's Club, count:12\n",
      "ent: Westgate, count:12\n",
      "ent: St Paul's Cathedral, count:12\n",
      "ent: Dens Park, count:12\n",
      "ent: Holyrood, count:11\n",
      "ent: Westminster Hall, count:11\n",
      "ent: Piccadilly, count:11\n",
      "ent: Guildhall, count:11\n",
      "ent: the SSE Arena, count:11\n",
      "ent: Kremlin, count:11\n",
      "ent: Bataclan, count:11\n",
      "ent: Market Street, count:11\n",
      "ent: the O2 Arena, count:11\n",
      "ent: King's Cross, count:11\n",
      "ent: Hampden, count:11\n",
      "\n",
      "========= GPE =========\n",
      "ent: UK, count:3588\n",
      "ent: US, count:2535\n",
      "ent: England, count:2273\n",
      "ent: London, count:2003\n",
      "ent: Scotland, count:1545\n",
      "ent: Wales, count:1392\n",
      "ent: Britain, count:1082\n",
      "ent: France, count:1010\n",
      "ent: Australia, count:841\n",
      "ent: China, count:765\n",
      "ent: Germany, count:738\n",
      "ent: Northern Ireland, count:728\n",
      "ent: Russia, count:653\n",
      "ent: India, count:550\n",
      "ent: Italy, count:493\n",
      "ent: Syria, count:488\n",
      "ent: Ireland, count:480\n",
      "ent: Glasgow, count:471\n",
      "ent: Spain, count:419\n",
      "ent: Edinburgh, count:416\n",
      "ent: South Africa, count:412\n",
      "ent: Paris, count:407\n",
      "ent: the United States, count:400\n",
      "ent: New York, count:400\n",
      "ent: Cardiff, count:377\n",
      "ent: Belfast, count:376\n",
      "ent: Japan, count:372\n",
      "ent: New Zealand, count:364\n",
      "ent: Birmingham, count:348\n",
      "ent: Turkey, count:321\n",
      "ent: Manchester, count:311\n",
      "ent: America, count:311\n",
      "ent: Iraq, count:306\n",
      "ent: Canada, count:306\n",
      "ent: Brazil, count:293\n",
      "ent: Washington, count:280\n",
      "ent: Pakistan, count:265\n",
      "ent: Netherlands, count:242\n",
      "ent: Afghanistan, count:239\n",
      "ent: California, count:230\n",
      "ent: Belgium, count:228\n",
      "ent: Brussels, count:216\n",
      "ent: Liverpool, count:215\n",
      "ent: Ukraine, count:211\n",
      "ent: Beijing, count:208\n",
      "ent: Sweden, count:203\n",
      "ent: Moscow, count:203\n",
      "ent: Iran, count:200\n",
      "ent: Nigeria, count:196\n",
      "ent: Bristol, count:190\n",
      "ent: Kent, count:188\n",
      "ent: Mexico, count:188\n",
      "ent: Greece, count:187\n",
      "ent: Dublin, count:186\n",
      "ent: Poland, count:183\n",
      "ent: Argentina, count:180\n",
      "ent: Israel, count:177\n",
      "ent: Essex, count:169\n",
      "ent: Switzerland, count:167\n",
      "ent: Sheffield, count:167\n",
      "ent: Lancashire, count:163\n",
      "ent: Swansea, count:162\n",
      "ent: Portugal, count:161\n",
      "ent: Egypt, count:159\n",
      "ent: Rio, count:159\n",
      "ent: Northern Ireland's, count:158\n",
      "ent: South Korea, count:151\n",
      "ent: Great Britain, count:148\n",
      "ent: Norway, count:147\n",
      "ent: Yorkshire, count:147\n",
      "ent: Leeds, count:146\n",
      "ent: USA, count:145\n",
      "ent: Kenya, count:144\n",
      "ent: the Republic of Ireland, count:140\n",
      "ent: Hong Kong, count:138\n",
      "ent: Aberdeen, count:132\n",
      "ent: Saudi Arabia, count:132\n",
      "ent: Sydney, count:130\n",
      "ent: Hampshire, count:129\n",
      "ent: Devon, count:127\n",
      "ent: Austria, count:124\n",
      "ent: Denmark, count:123\n",
      "ent: Cornwall, count:122\n",
      "ent: Los Angeles, count:121\n",
      "ent: Derbyshire, count:120\n",
      "ent: Singapore, count:120\n",
      "ent: Florida, count:120\n",
      "ent: Bangladesh, count:118\n",
      "ent: Berlin, count:116\n",
      "ent: Surrey, count:113\n",
      "ent: Newcastle, count:112\n",
      "ent: Nottingham, count:112\n",
      "ent: Libya, count:110\n",
      "ent: Newport, count:109\n",
      "ent: Hollywood, count:107\n",
      "ent: Inverness, count:105\n",
      "ent: Texas, count:104\n",
      "ent: Somerset, count:102\n",
      "ent: Sri Lanka, count:101\n",
      "ent: South Africa's, count:99\n",
      "\n",
      "========= NORP =========\n",
      "ent: British, count:1589\n",
      "ent: European, count:910\n",
      "ent: Scottish, count:880\n",
      "ent: American, count:721\n",
      "ent: French, count:690\n",
      "ent: Welsh, count:681\n",
      "ent: Conservative, count:583\n",
      "ent: German, count:570\n",
      "ent: Russian, count:473\n",
      "ent: Chinese, count:454\n",
      "ent: English, count:452\n",
      "ent: Australian, count:404\n",
      "ent: Irish, count:393\n",
      "ent: Conservatives, count:322\n",
      "ent: Italian, count:321\n",
      "ent: Muslim, count:298\n",
      "ent: Labour, count:277\n",
      "ent: Syrian, count:270\n",
      "ent: Tory, count:256\n",
      "ent: Indian, count:253\n",
      "ent: Islamist, count:246\n",
      "ent: Americans, count:218\n",
      "ent: Republican, count:216\n",
      "ent: Spanish, count:210\n",
      "ent: Tories, count:207\n",
      "ent: African, count:201\n",
      "ent: Turkish, count:195\n",
      "ent: Japanese, count:186\n",
      "ent: Dutch, count:183\n",
      "ent: Muslims, count:182\n",
      "ent: Canadian, count:168\n",
      "ent: Briton, count:168\n",
      "ent: Islamic, count:166\n",
      "ent: Lib Dem, count:152\n",
      "ent: Liberal Democrat, count:151\n",
      "ent: Brazilian, count:149\n",
      "ent: Asian, count:147\n",
      "ent: South African, count:143\n",
      "ent: Scots, count:137\n",
      "ent: Scot, count:129\n",
      "ent: Belgian, count:128\n",
      "ent: Christian, count:124\n",
      "ent: Israeli, count:122\n",
      "ent: Swiss, count:121\n",
      "ent: Jewish, count:116\n",
      "ent: Greek, count:112\n",
      "ent: Iraqi, count:107\n",
      "ent: Democratic, count:103\n",
      "ent: Arab, count:101\n",
      "ent: Iranian, count:100\n",
      "ent: Nazi, count:99\n",
      "ent: Republicans, count:99\n",
      "ent: Kurdish, count:98\n",
      "ent: Mexican, count:97\n",
      "ent: Afghan, count:95\n",
      "ent: Egyptian, count:95\n",
      "ent: Wales, count:94\n",
      "ent: Polish, count:94\n",
      "ent: Portuguese, count:91\n",
      "ent: Britons, count:90\n",
      "ent: Catholic, count:88\n",
      "ent: Frenchman, count:87\n",
      "ent: Pakistani, count:86\n",
      "ent: Russians, count:86\n",
      "ent: Saudi, count:85\n",
      "ent: Ukrainian, count:85\n",
      "ent: Englishman, count:85\n",
      "ent: Nigerian, count:83\n",
      "ent: Swedish, count:77\n",
      "ent: Shia, count:77\n",
      "ent: Jews, count:75\n",
      "ent: Argentine, count:75\n",
      "ent: the Lib Dems, count:75\n",
      "ent: Norwegian, count:73\n",
      "ent: Palestinian, count:72\n",
      "ent: Soviet, count:72\n",
      "ent: Kenyan, count:72\n",
      "ent: Germans, count:70\n",
      "ent: Austrian, count:67\n",
      "ent: South Korean, count:66\n",
      "ent: Czech, count:66\n",
      "ent: Nazis, count:64\n",
      "ent: Danish, count:64\n",
      "ent: Palestinians, count:64\n",
      "ent: Christians, count:63\n",
      "ent: Australians, count:62\n",
      "ent: Sunni, count:61\n",
      "ent: Hindu, count:56\n",
      "ent: Democrats, count:56\n",
      "ent: Indians, count:55\n",
      "ent: Syrians, count:54\n",
      "ent: Welshman, count:54\n",
      "ent: Colombian, count:51\n",
      "ent: the Liberal Democrats, count:49\n",
      "ent: New Zealander, count:48\n",
      "ent: North Korean, count:48\n",
      "ent: England, count:48\n",
      "ent: Europeans, count:48\n",
      "ent: Northern Irishman, count:47\n",
      "ent: Islamists, count:47\n",
      "\n",
      "========= LOC =========\n",
      "ent: Europe, count:1093\n",
      "ent: Africa, count:290\n",
      "ent: the Middle East, count:186\n",
      "ent: Asia, count:178\n",
      "ent: Earth, count:153\n",
      "ent: Atlantic, count:84\n",
      "ent: North America, count:67\n",
      "ent: West, count:66\n",
      "ent: Highlands, count:65\n",
      "ent: Mediterranean, count:62\n",
      "ent: Gulf, count:57\n",
      "ent: Pacific, count:52\n",
      "ent: eurozone, count:50\n",
      "ent: Middle East, count:50\n",
      "ent: South America, count:48\n",
      "ent: Latin America, count:43\n",
      "ent: Midlands, count:42\n",
      "ent: Caribbean, count:41\n",
      "ent: North Africa, count:41\n",
      "ent: the North Sea, count:38\n",
      "ent: the Black Country, count:38\n",
      "ent: Arctic, count:37\n",
      "ent: North, count:31\n",
      "ent: West End, count:31\n",
      "ent: Mars, count:29\n",
      "ent: North Sea, count:29\n",
      "ent: West Africa, count:29\n",
      "ent: Kashmir, count:29\n",
      "ent: Silicon Valley, count:28\n",
      "ent: Sun, count:27\n",
      "ent: Balkans, count:26\n",
      "ent: Americas, count:25\n",
      "ent: Borders, count:25\n",
      "ent: the South China Sea, count:24\n",
      "ent: South, count:23\n",
      "ent: Highland, count:23\n",
      "ent: the North East, count:23\n",
      "ent: Channel, count:23\n",
      "ent: the East Midlands, count:22\n",
      "ent: sub-Saharan Africa, count:22\n",
      "ent: Clyde, count:20\n",
      "ent: South East Asia, count:20\n",
      "ent: the South West, count:20\n",
      "ent: the Isle of Wight, count:19\n",
      "ent: Central America, count:19\n",
      "ent: West Midlands, count:18\n",
      "ent: the West Midlands, count:18\n",
      "ent: the South East, count:18\n",
      "ent: the North West, count:18\n",
      "ent: the Western Isles, count:18\n",
      "ent: Eastern Europe, count:17\n",
      "ent: the Indian Ocean, count:16\n",
      "ent: East Midlands, count:16\n",
      "ent: Skye, count:16\n",
      "ent: the north east, count:16\n",
      "ent: the River Thames, count:16\n",
      "ent: the Irish Sea, count:15\n",
      "ent: East, count:15\n",
      "ent: Cardiff Bay, count:15\n",
      "ent: the English Channel, count:15\n",
      "ent: Premiership, count:14\n",
      "ent: Universe, count:14\n",
      "ent: North East, count:14\n",
      "ent: Western Europe, count:14\n",
      "ent: Sinai, count:14\n",
      "ent: North West, count:14\n",
      "ent: the West End, count:14\n",
      "ent: the Far East, count:14\n",
      "ent: Indian Ocean, count:13\n",
      "ent: Everest, count:13\n",
      "ent: Eurozone, count:13\n",
      "ent: Antarctic, count:13\n",
      "ent: Thames, count:13\n",
      "ent: Himalayas, count:13\n",
      "ent: the Lake District, count:12\n",
      "ent: earth, count:12\n",
      "ent: Scandinavia, count:12\n",
      "ent: Moon, count:12\n",
      "ent: East Africa, count:12\n",
      "ent: the Gulf of Mexico, count:12\n",
      "ent: Antarctica, count:12\n",
      "ent: Western, count:12\n",
      "ent: Jupiter, count:12\n",
      "ent: the Black Sea, count:12\n",
      "ent: Red Sea, count:11\n",
      "ent: Amazon, count:11\n",
      "ent: Siberia, count:10\n",
      "ent: Shetland, count:10\n",
      "ent: Forth, count:10\n",
      "ent: South China Sea, count:9\n",
      "ent: Hyde Park, count:9\n",
      "ent: Mount Everest, count:9\n",
      "ent: South Asia, count:9\n",
      "ent: Central Asia, count:9\n",
      "ent: the Red Sea, count:9\n",
      "ent: Western Isles, count:9\n",
      "ent: Somme, count:9\n",
      "ent: East Asia, count:9\n",
      "ent: the Pacific Ocean, count:9\n",
      "ent: Valley, count:9\n",
      "\n",
      "========= EVENT =========\n",
      "ent: World Cup, count:391\n",
      "ent: Olympics, count:241\n",
      "ent: the World Cup, count:191\n",
      "ent: Euro 2016, count:161\n",
      "ent: FA Cup, count:153\n",
      "ent: Brexit, count:153\n",
      "ent: World War Two, count:152\n",
      "ent: Rio, count:145\n",
      "ent: Games, count:133\n",
      "ent: London 2012, count:105\n",
      "ent: Six Nations, count:102\n",
      "ent: Wimbledon, count:101\n",
      "ent: Grand Slam, count:90\n",
      "ent: the FA Cup, count:87\n",
      "ent: League Cup, count:79\n",
      "ent: Commonwealth Games, count:78\n",
      "ent: World Championships, count:74\n",
      "ent: the Six Nations, count:64\n",
      "ent: the World Championships, count:63\n",
      "ent: Championship, count:60\n",
      "ent: Masters, count:58\n",
      "ent: US Open, count:57\n",
      "ent: World War One, count:57\n",
      "ent: Ashes, count:56\n",
      "ent: Scottish Cup, count:54\n",
      "ent: Olympic Games, count:51\n",
      "ent: Euros, count:50\n",
      "ent: London, count:50\n",
      "ent: the US Open, count:45\n",
      "ent: Tour de France, count:45\n",
      "ent: World Cups, count:45\n",
      "ent: World War II, count:43\n",
      "ent: Open, count:43\n",
      "ent: Holocaust, count:41\n",
      "ent: Africa Cup of Nations, count:41\n",
      "ent: the Olympic Games, count:40\n",
      "ent: the Commonwealth Games, count:40\n",
      "ent: Paralympics, count:38\n",
      "ent: Rio 2016, count:38\n",
      "ent: First Half, count:38\n",
      "ent: Australian Open, count:38\n",
      "ent: the League Cup, count:37\n",
      "ent: the Australian Open, count:36\n",
      "ent: World Championship, count:35\n",
      "ent: the Rio Olympics, count:35\n",
      "ent: Troubles, count:34\n",
      "ent: Ryder Cup, count:31\n",
      "ent: Tour, count:31\n",
      "ent: the Tour de France, count:30\n",
      "ent: the 2014 World Cup, count:30\n",
      "ent: European Championship, count:30\n",
      "ent: European Champions Cup, count:30\n",
      "ent: French Open, count:29\n",
      "ent: the Rio Games, count:29\n",
      "ent: Challenge Cup, count:29\n",
      "ent: Champions Cup, count:28\n",
      "ent: Davis Cup, count:28\n",
      "ent: a Grand Slam, count:26\n",
      "ent: the Scottish Cup, count:26\n",
      "ent: the French Open, count:25\n",
      "ent: Second Half, count:25\n",
      "ent: the European Championships, count:24\n",
      "ent: a World Cup, count:24\n",
      "ent: Nations Cup, count:24\n",
      "ent: the T20 Blast, count:23\n",
      "ent: Grand Slams, count:23\n",
      "ent: European Championships, count:23\n",
      "ent: Rugby World Cup, count:23\n",
      "ent: Premiership, count:23\n",
      "ent: Rio Olympics, count:22\n",
      "ent: Pro12, count:22\n",
      "ent: European Cup, count:21\n",
      "ent: Euro 2012, count:21\n",
      "ent: the Challenge Cup, count:21\n",
      "ent: Olympic, count:21\n",
      "ent: T20 Blast, count:20\n",
      "ent: the Cold War, count:20\n",
      "ent: EFL Cup, count:20\n",
      "ent: Cold War, count:20\n",
      "ent: the EFL Cup, count:20\n",
      "ent: the London Olympics, count:20\n",
      "ent: All-Ireland, count:19\n",
      "ent: Commonwealth, count:19\n",
      "ent: Autumn Statement, count:18\n",
      "ent: Worlds, count:18\n",
      "ent: the Second World War, count:18\n",
      "ent: Beijing, count:18\n",
      "ent: the Africa Cup of Nations, count:17\n",
      "ent: the Nations Cup, count:17\n",
      "ent: the First World War, count:17\n",
      "ent: the 2015 World Cup, count:17\n",
      "ent: World Series, count:17\n",
      "ent: TT, count:17\n",
      "ent: 9/11, count:17\n",
      "ent: World Twenty20, count:17\n",
      "ent: Champions Trophy, count:17\n",
      "ent: the Champions Cup, count:17\n",
      "ent: Euro 2017, count:15\n",
      "ent: New Year, count:15\n",
      "ent: Division One, count:15\n",
      "\n",
      "========= LANGUAGE =========\n",
      "ent: English, count:257\n",
      "ent: Arabic, count:65\n",
      "ent: French, count:56\n",
      "ent: Welsh, count:55\n",
      "ent: Spanish, count:44\n",
      "ent: Chinese, count:24\n",
      "ent: Russian, count:22\n",
      "ent: German, count:21\n",
      "ent: Hindi, count:15\n",
      "ent: Mandarin, count:11\n",
      "ent: Irish, count:11\n",
      "ent: Portuguese, count:11\n",
      "ent: Italian, count:10\n",
      "ent: Swahili, count:8\n",
      "ent: Hebrew, count:7\n",
      "ent: Gaelic, count:7\n",
      "ent: Japanese, count:7\n",
      "ent: Polish, count:7\n",
      "ent: Latin, count:6\n",
      "ent: Tamil, count:5\n",
      "ent: Hausa, count:5\n",
      "ent: Urdu, count:5\n",
      "ent: Pashto, count:4\n",
      "ent: Farsi, count:4\n",
      "ent: Korean, count:4\n",
      "ent: Turkish, count:4\n",
      "ent: Ukrainian, count:4\n",
      "ent: Dutch, count:3\n",
      "ent: Bengali, count:3\n",
      "ent: Cantonese, count:3\n",
      "ent: Creole, count:3\n",
      "ent: Kurdish, count:3\n",
      "ent: Swedish, count:2\n",
      "ent: Dari, count:2\n",
      "ent: Persian, count:2\n",
      "ent: Amharic, count:2\n",
      "ent: Punjabi, count:2\n",
      "ent: Afrikaans, count:2\n",
      "ent: Cornish, count:2\n",
      "ent: Tswana, count:2\n",
      "ent: Braille, count:2\n",
      "ent: Malay, count:2\n",
      "ent: Pidgin, count:2\n",
      "ent: Yoruba, count:2\n",
      "ent: Nuer, count:2\n",
      "ent: Igbo, count:2\n",
      "ent: Telugu, count:2\n",
      "ent: Romanian, count:2\n",
      "ent: Nino, count:1\n",
      "ent: Maori, count:1\n",
      "ent: Anglo, count:1\n",
      "ent: Oromo, count:1\n",
      "ent: Navajo, count:1\n",
      "ent: Amharas, count:1\n",
      "ent: Copahue, count:1\n",
      "ent: Mapuche, count:1\n",
      "ent: Morse, count:1\n",
      "ent: Serbo, count:1\n",
      "ent: Senedd, count:1\n",
      "ent: Tshiluba, count:1\n",
      "ent: Noongar, count:1\n",
      "ent: Islamic, count:1\n",
      "ent: Tajik, count:1\n",
      "ent: Uzbek, count:1\n",
      "ent: Euro, count:1\n",
      "ent: Armenian, count:1\n",
      "ent: Shona, count:1\n",
      "ent: Rwandan, count:1\n",
      "ent: Halal, count:1\n",
      "ent: Inupiaq, count:1\n",
      "ent: Cynghanedd, count:1\n",
      "ent: cynghanedd, count:1\n",
      "ent: Mongolian, count:1\n",
      "ent: Nouchi, count:1\n",
      "ent: Kiswahili, count:1\n",
      "ent: Somali, count:1\n",
      "ent: Guarani, count:1\n",
      "ent: Sanskrit, count:1\n",
      "ent: Malayalam, count:1\n",
      "ent: Georgian, count:1\n",
      "ent: Nepali, count:1\n",
      "ent: Gujarati, count:1\n",
      "ent: Marathi, count:1\n",
      "ent: Tigrinya, count:1\n",
      "ent: Thai, count:1\n",
      "ent: Burmese, count:1\n",
      "ent: Indonesian, count:1\n",
      "ent: Vietnamese, count:1\n",
      "ent: Berber, count:1\n",
      "ent: Juba, count:1\n",
      "ent: Bari, count:1\n",
      "ent: Zande, count:1\n",
      "ent: Dinka, count:1\n",
      "ent: isiZulu, count:1\n",
      "ent: Latvian, count:1\n",
      "ent: Koran, count:1\n",
      "ent: Fijian, count:1\n",
      "ent: Catalan, count:1\n",
      "ent: Ghanaian, count:1\n",
      "ent: Temne, count:1\n",
      "\n",
      "========= LAW =========\n",
      "ent: Article 50, count:68\n",
      "ent: Sharia, count:25\n",
      "ent: the Good Friday Agreement, count:24\n",
      "ent: Brexit, count:21\n",
      "ent: the Lisbon Treaty, count:18\n",
      "ent: the Mental Health Act, count:16\n",
      "ent: the Freedom of Information Act, count:14\n",
      "ent: the European Convention on Human Rights, count:14\n",
      "ent: Schengen, count:13\n",
      "ent: the Scotland Bill, count:13\n",
      "ent: the Stormont House Agreement, count:12\n",
      "ent: the Autumn Statement, count:12\n",
      "ent: RHI, count:11\n",
      "ent: the Health and Safety at Work Act, count:11\n",
      "ent: the Wales Bill, count:10\n",
      "ent: Obamacare, count:9\n",
      "ent: Barnett, count:8\n",
      "ent: The Wales Bill, count:8\n",
      "ent: the Human Rights Act, count:8\n",
      "ent: Koran, count:8\n",
      "ent: TTIP, count:8\n",
      "ent: the Equality Act, count:7\n",
      "ent: the Proceeds of Crime Act, count:7\n",
      "ent: Act, count:7\n",
      "ent: Minsk, count:7\n",
      "ent: the Affordable Care Act, count:7\n",
      "ent: the US Constitution, count:7\n",
      "ent: Freedom of Information, count:6\n",
      "ent: Constitution, count:6\n",
      "ent: the Paris Agreement, count:6\n",
      "ent: the Trans-Pacific Partnership, count:6\n",
      "ent: TPP, count:6\n",
      "ent: Nafta, count:6\n",
      "ent: the Terrorism Act, count:6\n",
      "ent: the Data Protection Act, count:5\n",
      "ent: the Second Amendment, count:5\n",
      "ent: Affordable Care Act, count:5\n",
      "ent: Article 2, count:4\n",
      "ent: the Offensive Behaviour at Football and Threatening Communications Act, count:4\n",
      "ent: the Public Health Bill, count:4\n",
      "ent: the North American Free Trade Agreement, count:4\n",
      "ent: the Rome Statute, count:4\n",
      "ent: the Companies Act, count:4\n",
      "ent: Royal Charter, count:4\n",
      "ent: the Dangerous Dogs Act, count:4\n",
      "ent: the Representation of the People Act, count:4\n",
      "ent: Article 50 of the Lisbon Treaty, count:4\n",
      "ent: the European Union (Notification of Withdrawal), count:4\n",
      "ent: the Trade Union Bill, count:3\n",
      "ent: the Investigatory Powers Bill, count:3\n",
      "ent: Equality Act, count:3\n",
      "ent: the Spending, count:3\n",
      "ent: the Criminal Finances Bill, count:3\n",
      "ent: the Digital Economy Bill, count:3\n",
      "ent: the Merchant Shipping (Homosexual Conduct) Bill, count:3\n",
      "ent: the Marriage (Same Sex Couples) Bill, count:3\n",
      "ent: the European Convention of Human Rights, count:3\n",
      "ent: Bill, count:3\n",
      "ent: Higher Education Governance Bill, count:3\n",
      "ent: Refugee Convention, count:3\n",
      "ent: the Higher Education and Research Bill, count:3\n",
      "ent: the EU Referendum Bill, count:3\n",
      "ent: Good Friday Agreement, count:3\n",
      "ent: Pro 12, count:3\n",
      "ent: Clause IV, count:3\n",
      "ent: Agreement, count:3\n",
      "ent: the Article 50 Bill, count:3\n",
      "ent: sharia, count:3\n",
      "ent: Investigatory Powers Bill, count:3\n",
      "ent: Stormont House Agreement, count:3\n",
      "ent: the Higher Education Bill, count:3\n",
      "ent: the Wildlife and Countryside Act, count:3\n",
      "ent: the Psychoactive Substances Bill, count:3\n",
      "ent: Psychoactive Substances Bill, count:3\n",
      "ent: the Animal Welfare Act 2006, count:3\n",
      "ent: the European Communities Act 1972, count:3\n",
      "ent: Formula 1, count:3\n",
      "ent: the Computer Misuse Act, count:3\n",
      "ent: the Terrorism Act 2006, count:3\n",
      "ent: The 1961, count:3\n",
      "ent: the Assisted Dying Bill, count:3\n",
      "ent: the Wildlife and Countryside Act 1981, count:3\n",
      "ent: Autumn Statement, count:3\n",
      "ent: the Suicide Act 1961, count:3\n",
      "ent: Euro 6, count:3\n",
      "ent: the Curriculum for Excellence, count:3\n",
      "ent: the Local Government Act 2000, count:2\n",
      "ent: Trans Pacific Partnership, count:2\n",
      "ent: the Justice Bill, count:2\n",
      "ent: the All Writs Act, count:2\n",
      "ent: Geneva Communique, count:2\n",
      "ent: the Eighth Amendment, count:2\n",
      "ent: the 2010, count:2\n",
      "ent: Pool A, count:2\n",
      "ent: Top 14, count:2\n",
      "ent: the Regulation of Investigatory Powers Act, count:2\n",
      "ent: the Treaty of Rome, count:2\n",
      "ent: CETA, count:2\n",
      "ent: the Prisons and Courts Bill, count:2\n",
      "ent: a Ten Minute Rule, count:2\n",
      "\n",
      "========= ORG =========\n",
      "ent: BBC, count:3014\n",
      "ent: EU, count:945\n",
      "ent: Labour, count:916\n",
      "ent: BBC Sport, count:585\n",
      "ent: Facebook, count:584\n",
      "ent: Premier League, count:520\n",
      "ent: UN, count:466\n",
      "ent: BBC News, count:444\n",
      "ent: Championship, count:443\n",
      "ent: NHS, count:431\n",
      "ent: the Premier League, count:357\n",
      "ent: Chelsea, count:354\n",
      "ent: Liverpool, count:341\n",
      "ent: Islamic State, count:327\n",
      "ent: Parliament, count:322\n",
      "ent: Manchester United, count:321\n",
      "ent: Arsenal, count:313\n",
      "ent: the European Union, count:293\n",
      "ent: SNP, count:270\n",
      "ent: Twitter, count:269\n",
      "ent: Manchester City, count:267\n",
      "ent: IS, count:266\n",
      "ent: Reuters, count:263\n",
      "ent: BBC Scotland, count:255\n",
      "ent: Celtic, count:248\n",
      "ent: UKIP, count:243\n",
      "ent: Instagram, count:241\n",
      "ent: Premiership, count:238\n",
      "ent: Wales, count:231\n",
      "ent: AFP, count:226\n",
      "ent: Police Scotland, count:219\n",
      "ent: Westminster, count:214\n",
      "ent: Rangers, count:203\n",
      "ent: the High Court, count:202\n",
      "ent: Champions League, count:201\n",
      "ent: United, count:197\n",
      "ent: Everton, count:197\n",
      "ent: BBC Radio 5, count:191\n",
      "ent: Commons, count:189\n",
      "ent: YouTube, count:186\n",
      "ent: State, count:184\n",
      "ent: Tottenham, count:180\n",
      "ent: League One, count:178\n",
      "ent: Leicester, count:170\n",
      "ent: Google, count:169\n",
      "ent: Barcelona, count:163\n",
      "ent: Treasury, count:163\n",
      "ent: Real Madrid, count:161\n",
      "ent: City, count:155\n",
      "ent: Newcastle, count:155\n",
      "ent: Nato, count:154\n",
      "ent: BBC Radio 4's, count:153\n",
      "ent: Congress, count:152\n",
      "ent: Aberdeen, count:151\n",
      "ent: DUP, count:149\n",
      "ent: West Ham, count:147\n",
      "ent: the Press Association, count:146\n",
      "ent: Sunderland, count:146\n",
      "ent: bbcnewsents, count:143\n",
      "ent: League Two, count:142\n",
      "ent: the Champions League, count:137\n",
      "ent: BBC Wales, count:136\n",
      "ent: Apple, count:135\n",
      "ent: Fifa, count:135\n",
      "ent: England, count:133\n",
      "ent: Blues, count:132\n",
      "ent: FBI, count:128\n",
      "ent: Southampton, count:127\n",
      "ent: Swansea, count:127\n",
      "ent: ITV, count:127\n",
      "ent: al-Qaeda, count:125\n",
      "ent: the White House, count:124\n",
      "ent: Aston Villa, count:120\n",
      "ent: BBC One, count:118\n",
      "ent: Hearts, count:116\n",
      "ent: Crystal Palace, count:115\n",
      "ent: Plaid Cymru, count:115\n",
      "ent: Islam, count:114\n",
      "ent: Stormont, count:113\n",
      "ent: Bournemouth, count:110\n",
      "ent: the House of Commons, count:108\n",
      "ent: Burnley, count:106\n",
      "ent: Mercedes, count:105\n",
      "ent: the Supreme Court, count:103\n",
      "ent: PSNI, count:103\n",
      "ent: Wigan, count:102\n",
      "ent: Holyrood, count:102\n",
      "ent: Leeds, count:101\n",
      "ent: the Labour Party, count:99\n",
      "ent: Guardian, count:99\n",
      "ent: Super League, count:98\n",
      "ent: Hull, count:98\n",
      "ent: the Welsh Government, count:98\n",
      "ent: Saints, count:97\n",
      "ent: Fulham, count:96\n",
      "ent: Taliban, count:94\n",
      "ent: Wasps, count:94\n",
      "ent: Associated Press, count:94\n",
      "ent: Reds, count:93\n",
      "ent: Watford, count:93\n"
     ]
    }
   ],
   "source": [
    "# a lil bit of humon labor\n",
    "for label in FILTER_LABELS:\n",
    "    print(f\"\\n========= {label} =========\")\n",
    "    for ent, count in list(concat_ent_pool_dict[label].items())[:100]:\n",
    "        print(f\"ent: {ent}, count:{count}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6623b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRECT_PAIRS = [(\"Brexit\", \"LAW\"),\n",
    "                  (\"Twitter\", \"ORG\"),\n",
    "                  (\"Â£2.5\", None),\n",
    "                  (\"Championship\", \"EVENT\"),\n",
    "                  (\"Euro 2016\", \"EVENT\"),\n",
    "                  (\"Formula 1\", \"EVENT\"),\n",
    "                  (\"Challenge Cup\", \"EVENT\"),\n",
    "                  (\"French Open\", \"EVENT\"),\n",
    "                  (\"PhD\", None),\n",
    "                  (\"Ofqual\", \"ORG\"),\n",
    "                  (\"headliners\", None),\n",
    "                  (\"TfL\", \"ORG\"),\n",
    "                  (\"Dembele\", \"PERSON\"),\n",
    "                  (\"Worlds\", \"EVENT\"),\n",
    "                  (\"the FA Trophy\", \"EVENT\"),\n",
    "                  (\"DfE\", \"ORG\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a761f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_labeling(ent_pool_dict, correct_pairs, all_labels):\n",
    "    for ent, correct_label in correct_pairs:\n",
    "        print(f\"======={ent}=======\")\n",
    "    \n",
    "        # find occurences\n",
    "        for label in all_labels:\n",
    "            ent_dict = ent_pool_dict[label]\n",
    "            if ent in ent_dict.keys():\n",
    "                print(f\"label: {label}, count: {ent_dict[ent]}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f33946bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_labeling_inplace(ent_pool_dict, correct_pairs):\n",
    "    for ent, correct_label in correct_pairs:\n",
    "        occurences = []\n",
    "\n",
    "        # find occurences\n",
    "        for label in ent_pool_dict.keys():\n",
    "            ent_dict = ent_pool_dict[label]\n",
    "            if ent in ent_dict.keys():\n",
    "                occurences.append((label, ent_dict[ent]))\n",
    "\n",
    "        # assign to right dict\n",
    "        fix_flag = False\n",
    "        \n",
    "        correct_count = np.sum([count for (_, count) in occurences])\n",
    "        for (label, _) in occurences:\n",
    "            if label == correct_label:  # update count\n",
    "                ent_pool_dict[label][ent] = correct_count\n",
    "                fix_flag = True\n",
    "            else:\n",
    "                del ent_pool_dict[label][ent]\n",
    "\n",
    "        # correct label not in occurences\n",
    "        if not fix_flag:\n",
    "            ent_pool_dict[label][ent] = correct_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4c45daa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======Brexit=======\n",
      "label: FAC, count: 3\n",
      "label: GPE, count: 19\n",
      "label: MONEY, count: 1\n",
      "label: ORG, count: 49\n",
      "label: PERSON, count: 4563\n",
      "label: WORK_OF_ART, count: 33\n",
      "\n",
      "=======Twitter=======\n",
      "label: FAC, count: 4\n",
      "label: NORP, count: 5\n",
      "label: ORG, count: 9357\n",
      "label: PERSON, count: 1148\n",
      "label: PRODUCT, count: 3483\n",
      "label: WORK_OF_ART, count: 11\n",
      "\n",
      "=======Â£2.5=======\n",
      "label: CARDINAL, count: 5\n",
      "label: LANGUAGE, count: 1\n",
      "label: NORP, count: 37\n",
      "label: PRODUCT, count: 4\n",
      "\n",
      "=======Championship=======\n",
      "label: DATE, count: 5\n",
      "label: EVENT, count: 338\n",
      "label: GPE, count: 8\n",
      "label: LAW, count: 266\n",
      "label: LOC, count: 88\n",
      "label: ORG, count: 1205\n",
      "label: PERSON, count: 8\n",
      "label: PRODUCT, count: 2\n",
      "\n",
      "=======Euro 2016=======\n",
      "label: DATE, count: 30\n",
      "label: EVENT, count: 24\n",
      "label: LAW, count: 235\n",
      "label: WORK_OF_ART, count: 2\n",
      "\n",
      "=======Formula 1=======\n",
      "label: LAW, count: 58\n",
      "label: ORG, count: 4\n",
      "\n",
      "=======Challenge Cup=======\n",
      "label: EVENT, count: 152\n",
      "label: LAW, count: 21\n",
      "label: ORG, count: 8\n",
      "label: PRODUCT, count: 1\n",
      "label: WORK_OF_ART, count: 4\n",
      "\n",
      "=======French Open=======\n",
      "label: EVENT, count: 86\n",
      "label: FAC, count: 6\n",
      "label: ORG, count: 9\n",
      "label: WORK_OF_ART, count: 62\n",
      "\n",
      "=======PhD=======\n",
      "label: ORG, count: 1\n",
      "label: WORK_OF_ART, count: 327\n",
      "\n",
      "=======Ofqual=======\n",
      "label: ORG, count: 42\n",
      "label: PERSON, count: 27\n",
      "label: WORK_OF_ART, count: 55\n",
      "\n",
      "=======headliners=======\n",
      "label: DATE, count: 2\n",
      "label: ORG, count: 3\n",
      "label: WORK_OF_ART, count: 58\n",
      "\n",
      "=======TfL=======\n",
      "label: FAC, count: 89\n",
      "label: ORG, count: 262\n",
      "label: PRODUCT, count: 54\n",
      "label: WORK_OF_ART, count: 81\n",
      "\n",
      "=======Dembele=======\n",
      "label: ORG, count: 34\n",
      "label: PERSON, count: 27\n",
      "label: PRODUCT, count: 9\n",
      "label: WORK_OF_ART, count: 115\n",
      "\n",
      "=======Worlds=======\n",
      "label: ORG, count: 13\n",
      "label: PRODUCT, count: 1\n",
      "label: WORK_OF_ART, count: 113\n",
      "\n",
      "=======the FA Trophy=======\n",
      "label: ORG, count: 1\n",
      "label: WORK_OF_ART, count: 28\n",
      "\n",
      "=======DfE=======\n",
      "label: DATE, count: 54\n",
      "label: FAC, count: 67\n",
      "label: LOC, count: 4\n",
      "label: ORG, count: 72\n",
      "label: WORK_OF_ART, count: 24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_labeling(ent_pool_dict=concat_ent_pool_dict,\n",
    "               correct_pairs=CORRECT_PAIRS,\n",
    "               all_labels=ALL_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0aeea41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_labeling_inplace(ent_pool_dict=concat_ent_pool_dict,\n",
    "                     correct_pairs=CORRECT_PAIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3894c7ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======Brexit=======\n",
      "label: WORK_OF_ART, count: 4668\n",
      "\n",
      "=======Twitter=======\n",
      "label: ORG, count: 14008\n",
      "\n",
      "=======Â£2.5=======\n",
      "label: PRODUCT, count: 47\n",
      "\n",
      "=======Championship=======\n",
      "label: EVENT, count: 1920\n",
      "\n",
      "=======Euro 2016=======\n",
      "label: EVENT, count: 291\n",
      "\n",
      "=======Formula 1=======\n",
      "label: ORG, count: 62\n",
      "\n",
      "=======Challenge Cup=======\n",
      "label: EVENT, count: 186\n",
      "\n",
      "=======French Open=======\n",
      "label: EVENT, count: 163\n",
      "\n",
      "=======PhD=======\n",
      "label: WORK_OF_ART, count: 328\n",
      "\n",
      "=======Ofqual=======\n",
      "label: ORG, count: 124\n",
      "\n",
      "=======headliners=======\n",
      "label: WORK_OF_ART, count: 63\n",
      "\n",
      "=======TfL=======\n",
      "label: ORG, count: 486\n",
      "\n",
      "=======Dembele=======\n",
      "label: PERSON, count: 185\n",
      "\n",
      "=======Worlds=======\n",
      "label: WORK_OF_ART, count: 127\n",
      "\n",
      "=======the FA Trophy=======\n",
      "label: WORK_OF_ART, count: 29\n",
      "\n",
      "=======DfE=======\n",
      "label: ORG, count: 221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_labeling(ent_pool_dict=concat_ent_pool_dict,\n",
    "               correct_pairs=CORRECT_PAIRS,\n",
    "               all_labels=ALL_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54429164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort dicts by count\n",
    "for label in ALL_LABELS:\n",
    "    concat_ent_pool_dict[label] = dict(sorted(concat_ent_pool_dict[label].items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a332ae10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to '../cache/concat_ent_pool_dict_pp.pkl'\n"
     ]
    }
   ],
   "source": [
    "save_to_cache_dir(concat_ent_pool_dict, \"concat_ent_pool_dict_pp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f41fca",
   "metadata": {},
   "source": [
    "## 3) preprocess duplicate entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afad3f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b1a8a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'../cache_trf/val_doc_ents_list.pkl' loaded\n",
      "'../cache_trf/val_sum_ents_list.pkl' loaded\n",
      "'../cache_trf/test_doc_ents_list.pkl' loaded\n",
      "'../cache_trf/test_sum_ents_list.pkl' loaded\n"
     ]
    }
   ],
   "source": [
    "# # train\n",
    "# train_doc_ents_list = load_from_cache_dir(\"train_doc_ents_list\")\n",
    "# train_sum_ents_list = load_from_cache_dir(\"train_sum_ents_list\")\n",
    "\n",
    "# # val\n",
    "# val_doc_ner_list = load_from_cache_dir(\"val_doc_ner_list\")\n",
    "# val_sum_ner_list = load_from_cache_dir(\"val_sum_ner_list\")\n",
    "val_doc_ents_list = load_from_cache_dir(\"val_doc_ents_list\", cache_dir)\n",
    "val_sum_ents_list = load_from_cache_dir(\"val_sum_ents_list\", cache_dir)\n",
    "\n",
    "# test\n",
    "# test_doc_ner_list = load_from_cache_dir(\"test_doc_ner_list\")\n",
    "# test_sum_ner_list = load_from_cache_dir(\"test_sum_ner_list\")\n",
    "test_doc_ents_list = load_from_cache_dir(\"test_doc_ents_list\", cache_dir)\n",
    "test_sum_ents_list = load_from_cache_dir(\"test_sum_ents_list\", cache_dir)\n",
    "\n",
    "# entities pool\n",
    "concat_ent_pool_dict = load_from_cache_dir(\"concat_ent_pool_dict\", cache_dir)\n",
    "# test_ent_pool_dict = load_from_cache_dir(\"test_ent_pool_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08f0946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_duplicates(ents_list):\n",
    "    count = 0\n",
    "    for c in ents_list:\n",
    "        ents = [ent for ((ent, label), count) in c.items()]\n",
    "        if len(ents) > len(set(ents)):\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f787061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(src_ents_list, ent_pool=concat_ent_pool_dict):\n",
    "    for c in tqdm(src_ents_list):\n",
    "        ents = [ent for ((ent, label), count) in c.items()]\n",
    "\n",
    "        # check duplicates and save the indices    \n",
    "        dup_ents = {}\n",
    "        for ent_idx, ((ent, label), count) in enumerate(c.items()):\n",
    "            if ents.count(ent) > 1: # duplicate\n",
    "                if ent not in dup_ents.keys():\n",
    "                    dup_ents[ent] = {\"ent_idx\": [ent_idx],\n",
    "                                     \"label\": [label],\n",
    "                                     \"count\": [count]}\n",
    "                else:\n",
    "                    dup_ents[ent][\"ent_idx\"].append(ent_idx)\n",
    "                    dup_ents[ent][\"label\"].append(label)\n",
    "                    dup_ents[ent][\"count\"].append(count)\n",
    "\n",
    "        if len(dup_ents) == 0: # no duplicates\n",
    "            continue\n",
    "\n",
    "        # if duplicates exist, remove them\n",
    "        for ent, ent_info in dup_ents.items():\n",
    "            # print(\"\\nduplicate entity:\", ent)\n",
    "            # print(\"duplicate entity info:\", ent_info)\n",
    "            counts = np.array(ent_info[\"count\"])\n",
    "            unique_max_count = True if len(np.flatnonzero(counts == np.max(counts))) == 1 else False\n",
    "\n",
    "            # if there is a dominent label, unify to it\n",
    "            # else, (all labels have the same counts), check ent_pool and choose the most popular one\n",
    "            if unique_max_count:\n",
    "                # print(\"unique max count exists\")\n",
    "                survive_idx = counts.argmax()\n",
    "            else:\n",
    "                # find the most popular one from entity pool\n",
    "                # print(\"choose from pool\")\n",
    "                labels = ent_info[\"label\"]\n",
    "                pool_counts = np.array([Counter(ent_pool[label])[ent] for label in labels])\n",
    "                # print(\"pool_counts\", pool_counts)\n",
    "                survive_idx = pool_counts.argmax()\n",
    "                \n",
    "            \n",
    "            # remove duplicates\n",
    "            survived = (ent, ent_info[\"label\"][survive_idx])\n",
    "            absorbed_list = [(ent, ent_info[\"label\"][absorb_idx])\n",
    "                             for absorb_idx in range(len(counts)) if absorb_idx != survive_idx]\n",
    "\n",
    "            # print(survived, c[survived])\n",
    "            # print(absorbed_list)\n",
    "            for absorbed in absorbed_list:\n",
    "                c[survived] += c[absorbed]\n",
    "                del c[absorbed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0868e697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val document: duplicate count - 2161\n",
      "val summary: duplicate count - 3\n",
      "test document: duplicate count - 2127\n",
      "test document: duplicate count - 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"val document: duplicate count - {count_duplicates(val_doc_ents_list)}\") \n",
    "print(f\"val summary: duplicate count - {count_duplicates(val_sum_ents_list)}\") \n",
    "print(f\"test document: duplicate count - {count_duplicates(test_doc_ents_list)}\") \n",
    "print(f\"test document: duplicate count - {count_duplicates(test_sum_ents_list)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d01f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_doc_ents_list_no_dup = copy.deepcopy(val_doc_ents_list)\n",
    "val_sum_ents_list_no_dup = copy.deepcopy(val_sum_ents_list)\n",
    "test_doc_ents_list_no_dup = copy.deepcopy(test_doc_ents_list)\n",
    "test_sum_ents_list_no_dup = copy.deepcopy(test_sum_ents_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4361e513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91897e566e94a158cce2ec39f6e170d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3efbc49b4d84df3831b5707074b7788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27246076cc94a0db1f413de8d0fbfe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92739486dc67427f97f6b7621530538e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "remove_duplicates(val_doc_ents_list_no_dup)\n",
    "remove_duplicates(val_sum_ents_list_no_dup)\n",
    "remove_duplicates(test_doc_ents_list_no_dup)\n",
    "remove_duplicates(test_sum_ents_list_no_dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c3d1d798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val document: duplicate count - 0\n",
      "val summary: duplicate count - 0\n",
      "test document: duplicate count - 0\n",
      "test document: duplicate count - 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"val document: duplicate count - {count_duplicates(val_doc_ents_list_no_dup)}\") \n",
    "print(f\"val summary: duplicate count - {count_duplicates(val_sum_ents_list_no_dup)}\") \n",
    "print(f\"test document: duplicate count - {count_duplicates(test_doc_ents_list_no_dup)}\") \n",
    "print(f\"test document: duplicate count - {count_duplicates(test_sum_ents_list_no_dup)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02ab24e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to '../cache_trf/val_doc_ents_list_no_dup.pkl'\n",
      "saved to '../cache_trf/val_sum_ents_list_no_dup.pkl'\n",
      "saved to '../cache_trf/test_doc_ents_list_no_dup.pkl'\n",
      "saved to '../cache_trf/test_sum_ents_list_no_dup.pkl'\n"
     ]
    }
   ],
   "source": [
    "save_to_cache_dir(val_doc_ents_list_no_dup, \"val_doc_ents_list_no_dup\", cache_dir)\n",
    "save_to_cache_dir(val_sum_ents_list_no_dup, \"val_sum_ents_list_no_dup\", cache_dir)\n",
    "save_to_cache_dir(test_doc_ents_list_no_dup, \"test_doc_ents_list_no_dup\", cache_dir)\n",
    "save_to_cache_dir(test_sum_ents_list_no_dup, \"test_sum_ents_list_no_dup\", cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98075b59",
   "metadata": {},
   "source": [
    "### 3-1) Preprocess entities list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7269d520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d96f7e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRECT_PAIRS = [(\"Brexit\", \"LAW\"),\n",
    "                  (\"Twitter\", \"ORG\"),\n",
    "                  (\"Â£2.5\", None),\n",
    "                  (\"Championship\", \"EVENT\"),\n",
    "                  (\"Euro 2016\", \"EVENT\"),\n",
    "                  (\"Formula 1\", \"EVENT\"),\n",
    "                  (\"Challenge Cup\", \"EVENT\"),\n",
    "                  (\"French Open\", \"EVENT\"),\n",
    "                  (\"PhD\", None),\n",
    "                  (\"Ofqual\", \"ORG\"),\n",
    "                  (\"headliners\", None),\n",
    "                  (\"TfL\", \"ORG\"),\n",
    "                  (\"Dembele\", \"PERSON\"),\n",
    "                  (\"Worlds\", \"EVENT\"),\n",
    "                  (\"the FA Trophy\", \"EVENT\"),\n",
    "                  (\"DfE\", \"ORG\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f2aa8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'../cache/val_doc_ents_list_no_dup.pkl' loaded\n",
      "'../cache/val_sum_ents_list_no_dup.pkl' loaded\n",
      "'../cache/test_doc_ents_list_no_dup.pkl' loaded\n",
      "'../cache/test_sum_ents_list_no_dup.pkl' loaded\n",
      "'../cache/concat_ent_pool_dict_pp.pkl' loaded\n"
     ]
    }
   ],
   "source": [
    "# # train\n",
    "# train_doc_ents_list = load_from_cache_dir(\"train_doc_ents_list\")\n",
    "# train_sum_ents_list = load_from_cache_dir(\"train_sum_ents_list\")\n",
    "\n",
    "# # val\n",
    "# val_doc_ner_list = load_from_cache_dir(\"val_doc_ner_list\")\n",
    "# val_sum_ner_list = load_from_cache_dir(\"val_sum_ner_list\")\n",
    "val_doc_ents_list = load_from_cache_dir(\"val_doc_ents_list_no_dup\") # no duplicate\n",
    "val_sum_ents_list = load_from_cache_dir(\"val_sum_ents_list_no_dup\")\n",
    "\n",
    "# test\n",
    "# test_doc_ner_list = load_from_cache_dir(\"test_doc_ner_list\")\n",
    "# test_sum_ner_list = load_from_cache_dir(\"test_sum_ner_list\")\n",
    "test_doc_ents_list = load_from_cache_dir(\"test_doc_ents_list_no_dup\")\n",
    "test_sum_ents_list = load_from_cache_dir(\"test_sum_ents_list_no_dup\")\n",
    "\n",
    "# entities pool\n",
    "concat_ent_pool_dict = load_from_cache_dir(\"concat_ent_pool_dict_pp\") # preprocessed\n",
    "# test_ent_pool_dict = load_from_cache_dir(\"test_ent_pool_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e842b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_ents_list(src_ents_list, correct_pairs):    \n",
    "    replace_count = 0\n",
    "    delete_count = 0\n",
    "    \n",
    "    # new list\n",
    "    new_src_ents_list = []\n",
    "    \n",
    "    # entities to replace or delete\n",
    "    fix_ents = [ent for (ent, correct_label) in correct_pairs]\n",
    "    replace_dict = {ent: correct_label for (ent, correct_label) in correct_pairs if correct_label is not None}\n",
    "    replace_ents = replace_dict.keys()\n",
    "    \n",
    "    # enumerate over the list\n",
    "    for c in tqdm(src_ents_list):\n",
    "        new_c = copy.deepcopy(c)\n",
    "        for ((ent, label), count) in c.items():\n",
    "            if ent in fix_ents:\n",
    "                if ent in replace_ents:  # replace\n",
    "                    correct_label = replace_dict[ent]\n",
    "                    if label == correct_label:  # correct label\n",
    "                        pass\n",
    "                    else:  # incorrect label\n",
    "                        del new_c[(ent, label)]\n",
    "                        new_c.update({(ent, correct_label): count})\n",
    "                        replace_count += 1\n",
    "                else: # delete\n",
    "                    del new_c[(ent, label)]\n",
    "                    delete_count += 1\n",
    "        new_src_ents_list.append(new_c)\n",
    "    \n",
    "    print(\"replace_count:\", replace_count)\n",
    "    print(\"delete_count:\", delete_count)\n",
    "    \n",
    "    return new_src_ents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea20d911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f899c28d76d94adea2110db9d3ab0d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace_count: 548\n",
      "delete_count: 23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc3f301fedf4f76bf91f6f1beda46fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace_count: 108\n",
      "delete_count: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d4be30bb1740b1aa32a4b8c34eb0f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace_count: 562\n",
      "delete_count: 23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485b263df31d42a2a2459a45e2170695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace_count: 109\n",
      "delete_count: 0\n"
     ]
    }
   ],
   "source": [
    "val_doc_ents_list_pp = fix_ents_list(val_doc_ents_list, CORRECT_PAIRS)\n",
    "val_sum_ents_list_pp = fix_ents_list(val_sum_ents_list, CORRECT_PAIRS)\n",
    "test_doc_ents_list_pp = fix_ents_list(test_doc_ents_list, CORRECT_PAIRS)\n",
    "test_sum_ents_list_pp = fix_ents_list(test_sum_ents_list, CORRECT_PAIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0a2ac09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to '../cache/val_doc_ents_list_pp.pkl'\n",
      "saved to '../cache/val_sum_ents_list_pp.pkl'\n",
      "saved to '../cache/test_doc_ents_list_pp.pkl'\n",
      "saved to '../cache/test_sum_ents_list_pp.pkl'\n"
     ]
    }
   ],
   "source": [
    "save_to_cache_dir(val_doc_ents_list_pp, \"val_doc_ents_list_pp\")\n",
    "save_to_cache_dir(val_sum_ents_list_pp, \"val_sum_ents_list_pp\")\n",
    "save_to_cache_dir(test_doc_ents_list_pp, \"test_doc_ents_list_pp\")\n",
    "save_to_cache_dir(test_sum_ents_list_pp, \"test_sum_ents_list_pp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b38629",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6235329e4e80563f0f629f7d03bff3ed38888b52eb6adc0ce82f2c2e907be760"
  },
  "kernelspec": {
   "display_name": "xsum_analysis",
   "language": "python",
   "name": "xsum_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
