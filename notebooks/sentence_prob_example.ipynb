{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c89d0f79",
   "metadata": {},
   "source": [
    "https://discuss.huggingface.co/t/generation-probabilities-how-to-compute-probabilities-of-output-scores-for-gpt2/3175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08652df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5b05569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd19a95c2af6450197e02fa24bcf89f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5833133ad0d446d98a3666f6ede13c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/523M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47fe80bddf64f99a82e99ef33aec7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73cb7db8f8348bc950e7d7b3877c02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f341535f93a41c5b975eec4363837dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\", return_dict_in_generate=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "input_ids = tokenizer(\"Today is a nice day\", return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b105f4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 8888,   318,   257,  3621,  1110,   290, 11481,   477,   286,   674,\n",
       "          1751,   651,  1049,  6443,   287,  1204,    13,  1081,   257,  5875],\n",
       "        [ 8888,   318,   257,  3621,  1110,   284,   787,   257,  2877,   553,\n",
       "          1139,   406,  1071,   494,   569,   692,    83,    11,  7632,    11],\n",
       "        [ 8888,   318,   257,  3621,  1110,   329,   790,  2060,  1048,   508,\n",
       "           468,  6989,   281, 27357,    13,  1002,   345,   389,   319,   262]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_outputs = gpt2.generate(input_ids, do_sample=True, num_return_sequences=3, output_scores=True)\n",
    "generated_outputs.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96e6a12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 15])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only use id's that were generated\n",
    "# gen_sequences has shape [3, 15]\n",
    "gen_sequences = generated_outputs.sequences[:, input_ids.shape[-1]:]\n",
    "gen_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72bacf9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 15, 50257])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's stack the logits generated at each step to a tensor and transform\n",
    "# logits to probs\n",
    "probs = torch.stack(generated_outputs.scores, dim=1).softmax(-1)  # -> shape [3, 15, vocab_size]\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56a87ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0292, 0.0032, 0.0143, 0.4187, 0.0746, 0.0217, 0.0409, 0.0011, 0.0574,\n",
       "         0.0563, 0.4223, 0.3332, 0.0133, 0.1539, 0.0083],\n",
       "        [0.0843, 0.0108, 0.2449, 0.0232, 0.0147, 0.0859, 0.0235, 0.0102, 0.1379,\n",
       "         0.0037, 0.0266, 0.0066, 0.6087, 0.0070, 0.8728],\n",
       "        [0.2363, 0.0053, 0.1107, 0.3435, 0.2041, 0.1062, 0.0064, 0.0427, 0.0410,\n",
       "         0.2816, 0.0189, 0.6259, 0.1617, 0.0288, 0.3960]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we need to collect the probability of the generated token\n",
    "# we need to add a dummy dim in the end to make gather work\n",
    "gen_probs = torch.gather(probs, 2, gen_sequences[:, :, None]).squeeze(-1)\n",
    "gen_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29f17087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3284, 0.3352, 0.3363])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.logsumexp(gen_probs, dim=1) / torch.logsumexp(gen_probs, dim=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "792a4a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.4861e-04, 7.3639e-04, 9.9999e+01])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can do all kinds of things with the probs\n",
    "\n",
    "# 1) the probs that exactly those sequences are generated again\n",
    "# those are normally going to be very small\n",
    "unique_prob_per_sequence = gen_probs.prod(-1)\n",
    "unique_prob_per_sequence / unique_prob_per_sequence.sum(0) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f67eae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) normalize the probs over the three sequences\n",
    "normed_gen_probs = gen_probs / gen_probs.sum(0)\n",
    "assert normed_gen_probs[:, 0].sum() == 1.0, \"probs should be normalized\"\n",
    "\n",
    "# 3) compare normalized probs to each other like in 1)\n",
    "unique_normed_prob_per_sequence = normed_gen_probs.prod(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ac56e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.7732e-14, 1.6043e-13, 2.1785e-08])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_normed_prob_per_sequence = normed_gen_probs.prod(-1)\n",
    "unique_normed_prob_per_sequence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xsum_analysis",
   "language": "python",
   "name": "xsum_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
